import feedparser
import logging
import os
import sys
from typing import List, Dict, Any, Tuple
from datetime import datetime
import time
import re

from config import RSS_FEEDS, NEWS_PER_FEED

# Add the parent directory to path so we can import Flask models
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Try to import Flask models
try:
    from main import app, FeedSource, BotSettings
    use_db = True
    
    def get_feed_urls() -> List[str]:
        """Get feed URLs from the Flask database"""
        with app.app_context():
            feeds = FeedSource.query.filter_by(is_active=True).all()
            return [feed.url for feed in feeds]
            
    def get_news_limit() -> int:
        """Get number of news per source from settings"""
        with app.app_context():
            settings = BotSettings.query.first()
            if settings:
                return settings.news_per_source
            return NEWS_PER_FEED
except ImportError:
    # If Flask app is not available, use config directly
    use_db = False
    
    def get_feed_urls() -> List[str]:
        """Get feed URLs from config"""
        return RSS_FEEDS
        
    def get_news_limit() -> int:
        """Get number of news per source from config"""
        return NEWS_PER_FEED

def sanitize_html(text: str) -> str:
    """Remove unwanted HTML tags but keep basic formatting"""
    # Remove all HTML tags except allowed ones
    allowed_tags = ['b', 'i', 'u', 'a', 'code', 'pre']
    for tag in allowed_tags:
        text = text.replace(f"<{tag}>", f"[{tag}]").replace(f"</{tag}>", f"[/{tag}]")
    
    # Remove all other HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Restore allowed tags
    for tag in allowed_tags:
        text = text.replace(f"[{tag}]", f"<{tag}>").replace(f"[/{tag}]", f"</{tag}>")
    
    return text

def get_feed_entries(feed_url: str, max_entries: int = 3) -> List[Dict[str, Any]]:
    """Get entries from a specific RSS feed"""
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º User-Agent –≤ –∑–∞–ø—Ä–æ—Å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}
        feed = feedparser.parse(feed_url, request_headers=headers)
        
        if not feed or not hasattr(feed, 'entries') or not feed.entries:
            logger.warning(f"No entries found in feed {feed_url}")
            return []
            
        if hasattr(feed, 'bozo_exception') and feed.bozo:
            logger.warning(f"Partial error parsing feed {feed_url}: {feed.bozo_exception}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º, –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—å –∫–∞–∫–∏–µ-—Ç–æ entries
            if not feed.entries:
                return []
        
        entries = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–∏–∑ feed –∏–ª–∏ –∏–∑ URL)
        source_name = None
        if hasattr(feed, 'feed') and hasattr(feed.feed, 'title'):
            source_name = feed.feed.title
        if not source_name:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ URL
                if "lenta.ru" in feed_url:
                    source_name = "–õ–µ–Ω—Ç–∞.—Ä—É"
                elif "news.mail.ru" in feed_url:
                    source_name = "–ù–æ–≤–æ—Å—Ç–∏ Mail.ru"
                elif "russian.rt.com" in feed_url:
                    source_name = "RT –Ω–∞ —Ä—É—Å—Å–∫–æ–º"
                else:
                    # –ë–µ—Ä—ë–º –¥–æ–º–µ–Ω –∏–∑ URL
                    domain = feed_url.split('//')[-1].split('/')[0]
                    source_name = domain
            except:
                source_name = "–ù–æ–≤–æ—Å—Ç–∏"
        
        for i, entry in enumerate(feed.entries):
            if i >= max_entries:
                break
                
            # Extract required fields with fallbacks
            title = getattr(entry, 'title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
            
            # –ü–æ–ª—É—á–∞–µ–º URL –Ω–æ–≤–æ—Å—Ç–∏
            link = getattr(entry, 'link', '#')
            if not link or link == '#':
                link = getattr(entry, 'id', '#')
            
            # –ò—â–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö
            pub_date = None
            date_fields = ['published', 'pubDate', 'updated', 'date']
            for field in date_fields:
                if hasattr(entry, field) and getattr(entry, field):
                    try:
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å parsed-–≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–ª–µ–π –¥–ª—è –¥–∞—Ç—ã
                        parsed_field = f"{field}_parsed"
                        if hasattr(entry, parsed_field) and getattr(entry, parsed_field):
                            time_struct = getattr(entry, parsed_field)
                            pub_date = time.strftime("%d.%m.%Y %H:%M", time_struct)
                            break
                        # –ï—Å–ª–∏ –Ω–µ—Ç parsed-–ø–æ–ª—è, –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–æ–∫—É
                        date_str = getattr(entry, field)
                        if date_str:
                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            dt = datetime.strptime(date_str[:25], "%a, %d %b %Y %H:%M:%S")
                            pub_date = dt.strftime("%d.%m.%Y %H:%M")
                            break
                    except Exception as e:
                        logger.debug(f"Couldn't parse date: {e}")
                        continue
            
            # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö
            summary = None
            summary_fields = ['summary', 'description', 'content']
            for field in summary_fields:
                if hasattr(entry, field) and getattr(entry, field):
                    content = getattr(entry, field)
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–ª–µ–π –≤ RSS-—Ñ–∏–¥–∞—Ö
                    if isinstance(content, list) and content:
                        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∏–¥—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, Atom) —Ö—Ä–∞–Ω—è—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Å–ø–∏—Å–∫–µ —Å–ª–æ–≤–∞—Ä–µ–π
                        for item in content:
                            if isinstance(item, dict) and 'value' in item:
                                summary = item['value']
                                break
                    elif isinstance(content, str):
                        summary = content
                    
                    if summary:
                        break
            
            # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if not summary:
                summary = title
                
            # –û—á–∏—Å—Ç–∫–∞ HTML –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
            summary = sanitize_html(summary)
            if len(summary) > 300:
                summary = summary[:297] + "..."
            
            entries.append({
                'title': title,
                'link': link,
                'pub_date': pub_date,
                'summary': summary,
                'source': source_name
            })
            
        return entries
        
    except Exception as e:
        logger.error(f"Error fetching feed {feed_url}: {e}")
        return []

def get_latest_news(max_per_feed: int = 3) -> Tuple[List[Dict[str, Any]], bool]:
    """Get latest news from all configured RSS feeds"""
    all_news = []
    has_errors = False
    
    try:
        # Get feed URLs either from database or from config
        feed_urls = get_feed_urls()
        
        if not feed_urls:
            logger.warning("No feed URLs configured")
            return [], True
        
        for feed_url in feed_urls:
            entries = get_feed_entries(feed_url, max_per_feed)
            if not entries:
                has_errors = True
            all_news.extend(entries)
        
        # Sort by publication date (if available)
        all_news.sort(key=lambda x: x.get('pub_date', ''), reverse=True)
        
        return all_news, has_errors
        
    except Exception as e:
        logger.error(f"Error getting latest news: {e}")
        return [], True

def format_news_message(news_items: List[Dict[str, Any]]) -> str:
    """Format news items into a Telegram message"""
    if not news_items:
        return "üîç –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    news_by_source = {}
    for item in news_items:
        source = item.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
        if source not in news_by_source:
            news_by_source[source] = []
        news_by_source[source].append(item)
    
    all_parts = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å —Ç–µ–∫—É—â–∏–º –≤—Ä–µ–º–µ–Ω–µ–º –ø–æ –ú–æ—Å–∫–≤–µ
    try:
        # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω pytz, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ—Å–∫–æ–≤—Å–∫–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
        import pytz
        from datetime import timedelta
        moscow_tz = pytz.timezone('Europe/Moscow')
        now = datetime.now(pytz.UTC).astimezone(moscow_tz)
        time_string = f"{now.strftime('%d.%m.%Y %H:%M')} (MSK)"
    except ImportError:
        # –ò–Ω–∞—á–µ —Å—á–∏—Ç–∞–µ–º –≤—Ä—É—á–Ω—É—é UTC+3
        from datetime import timedelta
        now = datetime.now() + timedelta(hours=3)
        time_string = f"{now.strftime('%d.%m.%Y %H:%M')} (–ø–æ –ú–æ—Å–∫–≤–µ)"
    
    all_parts.append(f"üì∞ <b>–ù–û–í–û–°–¢–ò –ù–ê {time_string}</b>")
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É –Ω–æ–≤–æ—Å—Ç–µ–π
    for source, items in news_by_source.items():
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        source_header = f"üóû <b>{source}</b> ({len(items)})"
        all_parts.append(source_header)
        
        # –ù–æ–≤–æ—Å—Ç–∏ –∏–∑ —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        for item in items:
            title = item['title'].replace('<', '&lt;').replace('>', '&gt;')
            link = item['link']
            pub_date = f" ({item['pub_date']})" if item.get('pub_date') else ""
            
            # –£–ø—Ä–æ—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å HTML
            summary = item.get('summary', '')
            # –ò–∑–±–µ–≥–∞–µ–º –ø—Ä–æ–±–ª–µ–º —Å HTML —Ä–∞–∑–º–µ—Ç–∫–æ–π –ø—É—Ç–µ–º –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–≥–æ–≤
            summary = re.sub(r'<[^>]+>', '', summary)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è
            if len(summary) > 150:
                summary = summary[:147] + "..."
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç—å
            news_item = (
                f"üì∞ <b>{title}</b>{pub_date}\n"
                f"{summary}\n"
                f"üîó <a href='{link}'>–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é</a>"
            )
            all_parts.append(news_item)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏
        all_parts.append("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    
    # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
    if all_parts and all_parts[-1] == "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ":
        all_parts.pop()
    
    return "\n\n".join(all_parts)
